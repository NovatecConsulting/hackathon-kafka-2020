:toc:
:toc-title:
:toclevels: 3

:sectanchors:
:sectlinks:

:cp-version: 5.4.0

= Confluent Platform Community Edition Docker Environment

This folder contains a Docker-Compose environment with all components of the Confluent Platform Community Edition:

- Kafka (confluentinc/cp-kafka)
- Schema Registry (confluentinc/cp-schema-registry)
- Kafka Connect (confluentinc/cp-kafka-connect)
- KSqlDB Server (confluentinc/cp-ksql-server)

For more information see: https://docs.confluent.io/{cp-version}/installation/docker/image-reference.html

All those components are running in cluster mode.

Additionally it encompases Prometheus and Grafana as monitoring tools.

== Requirments

To start this environment, the following is required:

- Bash (for Windows use https://docs.microsoft.com/de-de/windows/wsl/install-win10[WSL] or https://cygwin.com/install.html[Cygwin])
- https://docs.docker.com/install/#server[Docker] (https://docs.microsoft.com/de-de/archive/blogs/stevelasker/configuring-docker-for-windows-volumes[in Windows volume mounts must be enabled])
- https://docs.docker.com/compose/install/[Docker-Compose]

== Quick Start

.Start complete CP environment
----
./up.sh --with-monitoring all
----

.Start CP environment with specific services
----
./up.sh --with-monitoring kafka schemaregistry connect ksqldb
----

.Stop complete CP environment
----
./down.sh
----

== Examples

Hint: _link:docker-run.sh[]_ is just a convenient script, which attaches the Docker network to the _docker run_ command. You may also execute _docker run --rm -it --net host confluentinc/cp-kafka:{cp-version} --bootstrap-server localhost:19092 ..._

=== Kafka commands

.Create and delete topics
[subs="attributes"]
----
./docker-run.sh confluentinc/cp-kafka:{cp-version} kafka-topics --bootstrap-server kafka:9092 --create --topic test --partitions 6 --replication-factor 3
./docker-run.sh confluentinc/cp-kafka:{cp-version} kafka-topics --bootstrap-server kafka:9092 --describe
./docker-run.sh confluentinc/cp-kafka:{cp-version} kafka-topics --bootstrap-server kafka:9092 --delete --topic test
----

.Produce records
[subs="attributes"]
----
./docker-run.sh confluentinc/cp-kafka:{cp-version} kafka-console-producer --broker-list kafka:9092 --property "parse.key=true" --property "key.separator=:" --topic test
----

.Once started, you can insert records
----
mykey:myvalue
anotherkey:somevalue
----

.Consume records
[subs="attributes"]
----
./docker-run.sh confluentinc/cp-kafka:{cp-version} kafka-console-consumer --bootstrap-server kafka:9092 --property "print.key=true" --property "print.timestamp=true" --from-beginning --topic test
----

=== Schema Registry commands

With the Confluent Schema Registry it is possible to produce and consume records with a Avro schema.
To find out how it works visit https://docs.confluent.io/{cp-version}/schema-registry/index.html.

.Produce records
[subs="attributes"]
----
./docker-run.sh confluentinc/cp-schema-registry:{cp-version} kafka-avro-console-producer --broker-list kafka:9092 --property schema.registry.url=http://schemaregistry:8081 --topic testavro --property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'
----

.Once started, you can insert records
----
{"f1": "value1"}
{"f1": "value2"}
{"f1": "value3"}
----

.Consume records
[subs="attributes"]
----
./docker-run.sh confluentinc/cp-schema-registry:{cp-version} kafka-avro-console-consumer --bootstrap-server kafka:9092 --property schema.registry.url=http://schemaregistry:8081 --from-beginning --topic testavro
----

It is also possible to directly interact with the Confluent Schema Registry via the Rest API.
The API is described at https://docs.confluent.io/{cp-version}/schema-registry/develop/api.html.

.Show subjects
----
./docker-run.sh dwdraju/alpine-curl-jq curl -s -H "Accept: application/vnd.schemaregistry.v1+json" http://schemaregistry:8081/subjects
----

.Show versions of registered subject
----
subject=testavro-value
docker-run.sh dwdraju/alpine-curl-jq curl -s -H "Accept: application/vnd.schemaregistry.v1+json" http://schemaregistry:8081/subjects/${subject}/versions/
----

.Show specific version of a schema within a subject
----
subject=testavro-value
schemaversion=1
docker-run.sh dwdraju/alpine-curl-jq curl -s -H "Accept: application/vnd.schemaregistry.v1+json" http://schemaregistry:8081/subjects/${subject}/versions/${schemaversion}
----

.Register a new schema for a subject
----
subject=mysubject
schema={"schema":"{\"type\":\"record\",\"name\":\"myschema\",\"namespace\":\"this.is.a.test\",\"fields\":[{\"name\":\"field\",\"type\":\"string\"}]}"}
./docker-run.sh dwdraju/alpine-curl-jq curl -s -XPOST -H "Content-Type: application/vnd.schemaregistry.v1+json" -H "Accept: application/vnd.schemaregistry.v1+json" --data "${schema}" http://schemaregistry:8081/subjects/${subject}/versions
----

.The registration command returns the id of the schema
----
[2]
----

.Get the schema by its id
----
schemaid=2
./docker-run.sh dwdraju/alpine-curl-jq curl -s -H "Accept: application/vnd.schemaregistry.v1+json" http://schemaregistry:8081/schemas/ids/${schemaid}
----

In order to use the Confluent Schema Registry in your application use the serializer and deserializer from the artifacts _io.confluent:kafka-avro-serializer_ and _io.confluent:kafka-streams-avro-serde_.

== Network and Credentials

[options="header"]
.Credentials
|===
| Service | Username | Password
| Grafana | admin | admin
|===

[cols="h,1"]
.Access to services within Docker network
|===
| Kafka Bootstrap Servers |  kafka:9092
| Schema Registry Urls | http://schemaregistry:8081
| Grafana Url | http://grafana:3000
| Prometheus Url | http://prometheus:9090
|===

[cols="h,1"]
.Access to services from host
|===
| Kafka Bootstrap Servers |  localhost:19092,localhost:29092,localhost:39092
| Schema Registry Urls | http://localhost:18081,http://localhost:28081
| Grafana Url | http://localhost:13000
| Prometheus Url | http://localhost:19090
|===
